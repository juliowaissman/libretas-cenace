{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBk0ZDWY-ff8"
   },
   "source": [
    "<center><H1>Primeros pasos en</H1><center>\n",
    "\n",
    "<center><img src=\"https://www.gstatic.com/devrel-devsite/prod/ve2848ad92313fddfcd40baeb58a2f663fe2fd55c371a714a6bb3e329e2b15223/tensorflow/images/lockup.svg\"  height=\"100px\" style=\"padding-bottom:5px;\"  /></center>\n",
    "\n",
    "<center><H2>Julio Waissman Vilanova</H2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"center\">\n",
    "      <td align=\"center\"><a target=\"_blank\" href=\"https://www.unison.mx\">\n",
    "            <img src=\"https://www.unison.mx/wp-content/themes/awaken/images/logo.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  /></a></td>  \n",
    "      <td align=\"center\"><a target=\"_blank\" href=\"https://www.gob.mx/cenace\">\n",
    "            <img src=\"https://universidad.cenace.gob.mx/pluginfile.php/244/block_html/content/CENACE-logo-completo.png\" width=\"300\" style=\"padding-bottom:5px;\" /></a></td>\n",
    "      <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/juliowaissman/rn-cenace/blob/main/cenace_tensorflow.ipynb\">\n",
    "            <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Ejecuta en Google Colab</a></td>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57knM8jrYZ2t"
   },
   "source": [
    "## 1 Instalar TensorFlow\n",
    "\n",
    "Tensorflow es un software libre desarrollado por google y usado en forma extensiva para tareas de aprendizaje máquina, principalmente con aprendizaje profundo. En esta primer libreta vamos a conocer tensorflow, como los cálculos son representados en Tensorflow y como definir una red neuronal simple.\n",
    "\n",
    "Vamos a basr el curso en Tensorflow 2.X, que es la versión más reciente y que provée gran flexibilidad y la habilidad de ejecutar las operaciones en forma explicita (sin necesidad de generar una sesión, como en Tensorflow 1.X).\n",
    "\n",
    "Vamos a ver que, si bien los calculos no se ejecutan en python, la sintaxis y la ejecución imperativa hacen que parezca casi transparente cuando se utiliza python y cuando se llama a Tensorflow.\n",
    "\n",
    "Vamos primero a instalar Tensorflow 2.x que es trivial, salvo que quieras hacer uso de GPUs, para lo que tienes que seguir la [guía de instalación](https://www.tensorflow.org/install?hl=es).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LkaimNJfYZ2w"
   },
   "outputs": [],
   "source": [
    "#%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "\n",
    "# Download and import the MIT 6.S191 package\n",
    "# !pip install mitdeeplearning\n",
    "# import mitdeeplearning as mdl\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QNMcdP4m3Vs"
   },
   "source": [
    "## 2 ¿Y porqué se llama Tensorflow?\n",
    "\n",
    "Tensorflow se llama así, porque su forma de operar es manejando un flujo de operaciones en tensores. Los tensores, como lo vimos en el curso, son estructuras de datos que podemos asociar a arreglos multidimensionales. Un tensor se representa como un arreglo n-dimensional de un tipo base de dato (como enteros, flotantes o cadenas de caracteres) y es una manera de generalizar las operaciones en vectores y matrices.\n",
    "\n",
    "El ```shape``` de un tensor (en tensorflow) define el número de dimensiones y el tamaño de cada una de ellas. El ```rank``` de un tensor nos proporciona el número de dimensiones del tensor (al que tambien se le llama orden o grado del tensor)\n",
    "\n",
    "Veamos ahora un tensor cero dimansional (un escalar):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFxztZQInlAB"
   },
   "outputs": [],
   "source": [
    "comida = tf.constant(\"chiles en nogada\", tf.string)\n",
    "pi = tf.constant(1.41421356237, tf.float64)\n",
    "\n",
    "print(f\"`comida` es un tensor {tf.rank(comida).numpy()}-d \")\n",
    "print(f\"`pi` es un tensor {tf.rank(pi).numpy()}-d \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dljcPUcoJZ6"
   },
   "source": [
    "Listas (de python) y `ndarray` (de numpy) pueden usarse para crear tensores de 1-d:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oaHXABe8oPcO"
   },
   "outputs": [],
   "source": [
    "comidas = tf.constant([\"chiles en nogada\", \"gallina pinta\"], tf.string)\n",
    "numeros = tf.constant([3.141592, 1.414213, 2.71821], tf.float64)\n",
    "\n",
    "print(f\"`comidas` es un tensor {tf.rank(comidas).numpy()}-d  con shape: {tf.shape(comidas)}\")\n",
    "print(f\"`numeros` es un tensor {tf.rank(numeros).numpy()}-d  con shape: {tf.shape(numeros)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gvffwkvtodLP"
   },
   "source": [
    "Ahora veamos como crear tensores 2-d (i.e. matrices) y tensores de más alto orden. Por ejemplo, más adelante vamos a utilizar tensores de 3 y 4 dimensiones como entrada para entrenar nuestros modelos neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tFeBBe1IouS3"
   },
   "outputs": [],
   "source": [
    "### Definir un tensor de alto orden ###\n",
    "\n",
    "'''TODO: Define un tensor 2-d'''\n",
    "matriz = # TODO\n",
    "\n",
    "assert isinstance(matriz, tf.Tensor)\n",
    "assert tf.rank(matriz).numpy() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zv1fTn_Ya_cz"
   },
   "outputs": [],
   "source": [
    "'''TODO: Definir un tensor4-d.'''\n",
    "# Usa tf.zeros para inicializar unb tensor 4-d de ceros con tamaño 10 x 256 x 256 x 3. \n",
    "# Puedes ver el tensor como una secuencia de 10 imágenes a color (RBG) de 256 por 256 pixeles.\n",
    "images = # TODO\n",
    "\n",
    "assert isinstance(images, tf.Tensor) \n",
    "assert tf.rank(images).numpy() == 4\n",
    "assert tf.shape(images).numpy().tolist() == [10, 256, 256, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkaCDOGapMyl"
   },
   "source": [
    "Como puedes ver, el  ```shape``` de un tensor nos da el número de elementos en cada dimensión. El ```shape``` es muy útil, y se usa bastante. \n",
    "\n",
    "Tambien es posible (en tensorflow 2.x) usar el *slicing* para acceder a *subtensores*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FhaufyObuLEG"
   },
   "outputs": [],
   "source": [
    "renglon_vector = matriz[1]\n",
    "columna_vector = matriz[:,2]\n",
    "escalar = matriz[1, 2]\n",
    "\n",
    "print(f\"`row_vector`: {renglon_vector.numpy()}\")\n",
    "print(f\"`column_vector`: {columna_vector.numpy()}\")\n",
    "print(f\"`scalar`: {escalar.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD3VO-LZYZ2z"
   },
   "source": [
    "## 3 Operaciones con tensores\n",
    "\n",
    "Una forma conveniente de pensar (y visualizar) las operaciones que se realizan en Tensorflow es representandolos en términos de gráficas (o grafos).\n",
    "\n",
    "Podemos definir una gráfica de operaciones como un modelo que representa los datos (en forma de tensores) y las operacones matemáticas que actuan en dichos tensores. Empecemos por ver algunos ejemplos simples y definamos dichas operaciones usando Tensorflow.\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/add-graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_YJrZsxYZ2z"
   },
   "outputs": [],
   "source": [
    "# Crea los nodos de la gráfica e inicializa valores\n",
    "a = tf.constant(15)\n",
    "b = tf.constant(61)\n",
    "\n",
    "# Desarrolla la operacion entre tensores\n",
    "c1 = tf.add(a,b)\n",
    "c2 = a + b # TensorFlow 2.x sobrecarga el operador \"+\" \n",
    "print(c1)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mbfv_QOiYZ23"
   },
   "source": [
    "Acabamos de realizar dos cosas diferentes: creamos una grafica de operaciones en tensores y luego hemos ejecutado la gráfica y nos ha devuelto su resultado. A esto se le conoce como computación floja o *lazy computation* y permite optimizar en forma bastante importante la operacion de Tensorflow.\n",
    "\n",
    "Consideremos ahora una gráfica ligeramente más complicada:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/computation-graph.png)\n",
    "\n",
    "En esta gráfica, se tienen dos tensores de entrada `a, b` y se calcula un tensor de salida `e`. Cada nodo en la gráfica representa una operación que toma entradas, realiza algun cálculo, y pasa su resultado a otro nodo.\n",
    "\n",
    "Vamos a definir una función sencilla en Tensorflow para construir esta gráfica de operaciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PJnfzpWyYZ23",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Definiendo una gráfica de operciones ###\n",
    "\n",
    "# Construue la función con el gráfico de operaciones de la imágen anterior\n",
    "def func(a,b):\n",
    "  '''TODO: Define las operaciones de c, d, e (usa tf.add, tf.subtract, tf.multiply).'''\n",
    "  c = # TODO\n",
    "  d = # TODO\n",
    "  e = # TODO\n",
    "  return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwrRfDMS2-oy"
   },
   "source": [
    "Ya está definido el grafo de operaciones, pero no se ha ejecutado. Vamos ahora a llamar a esta función para ejecutar la gráfica de operaciones para unas entradas `a,b` en particular:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnwsf8w2uF7p"
   },
   "outputs": [],
   "source": [
    "# Valores de ejemplo. \n",
    "# Mira como no fue necesario convertirlos a tensores\n",
    "a, b = 1.5, 2.5\n",
    "\n",
    "# Calcula la gráfica\n",
    "e_out = func(a,b)\n",
    "print(e_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6HqgUIUhYZ29"
   },
   "source": [
    "Se puede apreciar que la salida es un Tensor, con un valor definido por las operaciones de la gráfica. Igualmente se puede ver que la salida no tiene `shape`ya que es un escalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1h4o9Bb0YZ29"
   },
   "source": [
    "## 4 Redes neuronales en TensorFlow\n",
    "\n",
    "Uno de los principales usos de Tensorflow (pero no el único) es que permite desarrollar redes neuronales en un entorno simple que escala sin problema y con un mínimo conocimiento del hardware en el que se vaya a ejecutar. Tensorflow 2.x viene con [Keras](https://www.tensorflow.org/guide/keras), una API de alto nivel que provee un marco intuitivo y poderoso par el desarrollo y entrenamiento de modelos basados en redes neuronales profundas.\n",
    "\n",
    "Para ilustrar como se construye una red neuronal en Tensorflow, vamos a ver como ejemplo un simple perceptrón definido como una sola capa densa: $ y = \\sigma(Wx + b)$, donde $W$ es una matriz de pesos, $b$ es el sesgo, $x$ es la entrada (un vector en este caso), $\\sigma$ es la función de activación y por último $y$ es la salida. Esta operación cláramente puede verse como una gráfica de operaciones:\n",
    "\n",
    "![alt text](https://raw.githubusercontent.com/aamini/introtodeeplearning/master/lab1/img/computation-graph-2.png)\n",
    "\n",
    "Los tensores en Tensorflow pueden fluir utilizando una clase abstracta llamada [```Layers```](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) (el bloque básico para construir redes neuronales). La clase ```Layers``` implementa las operaciones más comunes para redes neuronales, y se usa para actualizar pesos, calcular pérdidas y definir la conectividad dentro de la misma capa.\n",
    "\n",
    "Vamos a definir a pie una clase de hereda de ```Layer``` para implementar nuestro perceptrón."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HutbJk-1kHPh"
   },
   "outputs": [],
   "source": [
    "### Definiendo una clase tipo Layer ###\n",
    "\n",
    "# n_output_nodes: número de nodos de salida\n",
    "# input_shape: shape de la entrada\n",
    "# x: entrada a la capa\n",
    "\n",
    "class NuestraDenseLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, n_output_nodes):\n",
    "    super(NuestraDenseLayer, self).__init__()\n",
    "    self.n_output_nodes = n_output_nodes\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    d = int(input_shape[-1])\n",
    "    # Define e inicializa parametros: W y b\n",
    "    # ¡Atención! Los pesos y bias son inicializados aleatoriamente\n",
    "    self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes]) # dimensiones\n",
    "    self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes]) # dimensiones\n",
    "\n",
    "  def call(self, x):\n",
    "    '''TODO: define la operación para z (tip: usa tf.matmul)'''\n",
    "    z = # TODO\n",
    "\n",
    "    '''TODO: define la operación para out (tip: use tf.sigmoid)'''\n",
    "    y = # TODO\n",
    "    return y\n",
    "\n",
    "# Como los pesos y bias son inicializados aleatoriamente,\n",
    "# vamos a fijar una semilla para que los resultados sean reproducibles\n",
    "tf.random.set_seed(1)\n",
    "layer = NuestraDenseLayer(3)\n",
    "layer.build((1,2))\n",
    "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
    "y = layer.call(x_input)\n",
    "\n",
    "# test \n",
    "print(y.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt1FgM7qYZ3D"
   },
   "source": [
    "En Keras existen una serie de ```Layers``` que son usadas muy comunmente en diferentes modelos de redes neuronales, como puede ser una capa tipo [```Dense```](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable).\n",
    "\n",
    "En lugar de estar desarrollando una serie de capas de tipo ```Layer``` simples para definir un modelo multicapa, vamos a utilizar la clase [`Sequential`](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/Sequential) de Keras y una capa [`Dense` ](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Dense) para definir nuestro modelo de perceptrón.\n",
    "\n",
    "Con la API de `Sequential` se pueden crear redes neuronales multicapa, simplemente aplilando capas como si fueran bloques de lego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7WXTpmoL6TDz"
   },
   "outputs": [],
   "source": [
    "### Definiendo una red neuronal con la API de Sequential ###\n",
    "\n",
    "# Importando el chuqui\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define el numero de salidas\n",
    "n_output_nodes = 3\n",
    "\n",
    "# Primero: definir el model Sequential \n",
    "model = Sequential()\n",
    "\n",
    "'''TODO: Define una capa densa (completamente conectada) para calcular z'''\n",
    "# Recuerda: las capas densas se definen por los parámetros W y b.\n",
    "# Se puede consultar más sobre la inicialización de W y b en\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense?version=stable\n",
    "dense_layer = # TODO\n",
    "\n",
    "# Add the dense layer to the model\n",
    "model.add(dense_layer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDGcwYfUyR-U"
   },
   "source": [
    "Y así de simple se define una red neuronal utilizando la API de ```Sequential```. Ahora podemos probar el modelo utilizando la mismisima entrada anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sg23OczByRDb"
   },
   "outputs": [],
   "source": [
    "# Probando con la misma entrada del modelo anterior\n",
    "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
    "\n",
    "'''TODO: Alimenta el modelo y predice la salida'''\n",
    "model_output = # TODO\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "596NvsOOtr9F"
   },
   "source": [
    "\n",
    "No solo eso, además de definir modelos usando la API de ```Sequential```, tambien es posible definir redes neuronales completas utilizando la clase[`Model`](https://www.tensorflow.org/api_docs/python/tf/keras/Model?version=stable), que agrupa un conjunto de capas juntas para facilitar el entrenamiento y la inferencia. La clase ```Model``` captura lo que se conoce como *modelo neuronal* o *red neuronal*. Creando una subclase de ```Model``` es posible crear una clase para un modelo desarrollado y definir el paso hacia adelante (*forward propagation*) a través de todas las capas usando la función `call`. Las subclases otorgan la flexibilidad para definir capas a la medida, ciclos de entrenamiento diseñados, funciones de activación propias y modelos propios. \n",
    "\n",
    "Vamos a definir la misma red neuronal que antes, pero ahora usando una subclase de ```Model``` en lugar de la API de ```Sequential```.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4aCflPVyViD"
   },
   "outputs": [],
   "source": [
    "### Definiendo un modelo a partir de Model ###\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class SubclassModel(tf.keras.Model):\n",
    "\n",
    "  # En __init__, se definen las capas del modelo\n",
    "  def __init__(self, n_output_nodes):\n",
    "    super(SubclassModel, self).__init__()\n",
    "    '''TODO: Our model consists of a single Dense layer. Define this layer.''' \n",
    "    self.dense_layer = '''TODO: Dense Layer'''\n",
    "\n",
    "  # En el método call, se define la propagación hacia adelante.\n",
    "  def call(self, inputs):\n",
    "    return self.dense_layer(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0-lwHDk4irB"
   },
   "source": [
    "Así como en el modelo contruido con la API de `Sequential`, vamos a probar nuestro nuevo modelo usando las mismas entradas anteriores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhB34RA-4gXb"
   },
   "outputs": [],
   "source": [
    "n_output_nodes = 3\n",
    "model = SubclassModel(n_output_nodes)\n",
    "\n",
    "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
    "\n",
    "print(model.call(x_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTIFMJLAzsyE"
   },
   "source": [
    "Es muy importante resaltar que la subclase de `Model` que se define nos permite mucha flexibilidad para definir nuestros propios modelos neuronales. Es posible ustilizar argumentos de tipo *booleano* en la función `call`para especificar diferentes comportamientos del modelo durante el entrenamiento y la inferencia. \n",
    "\n",
    "Por ejemplo, supongamos que, para ciertas circunstancias queremos que nuestro modelo simplemente pase la entrada directamente como salida. Para eso vamos a definir un argumento booleano para controlar este comportamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7jzGX5D1xT5"
   },
   "outputs": [],
   "source": [
    "### Definiendo un modelo con subclases de Model especificando comportamiento particular ###\n",
    "\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "class IdentityModel(tf.keras.Model):\n",
    "\n",
    "  # En __init__ se definen las capas del modelo\n",
    "  # Since our desired behavior involves the forward pass, this part is unchanged\n",
    "  def __init__(self, n_output_nodes):\n",
    "    super(IdentityModel, self).__init__()\n",
    "    self.dense_layer = tf.keras.layers.Dense(n_output_nodes, activation='sigmoid')\n",
    "\n",
    "  '''TODO: Implementa el comportamiento para el cual, \n",
    "           la salida es directamente la entrada, \n",
    "           si el argumento isidentity es verdadero.'''\n",
    "  def call(self, inputs, isidentity=False):\n",
    "    x = self.dense_layer(inputs)\n",
    "    '''TODO: Implementa el comportamiento de acuerdo al argumento'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ku4rcCGx5T3y"
   },
   "source": [
    "Vamos ahora a probarlo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NzC0mgbk5dp2"
   },
   "outputs": [],
   "source": [
    "n_output_nodes = 3\n",
    "model = IdentityModel(n_output_nodes)\n",
    "\n",
    "x_input = tf.constant([[1,2.]], shape=(1,2))\n",
    "'''TODO: pasa la entrada al modelo con el valor de isidentity falso y verdadero respectivamente'''\n",
    "out_activate = # TODO\n",
    "out_identity = # TODO\n",
    "\n",
    "print(f\"Salida con activación: {out_activate.numpy()}; Salida identidad: {out_identity.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7V1dEqdk6VI5"
   },
   "source": [
    "Ahora que ya sabemos definir capas con `Layes` y modelos neuronales usanto tando la API de `Sequential` como las subclases de `Model` vamos a ver como tensorflow facilita implementar los métodos de aprendizaje con \n",
    "*b-prop*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQwDhKn8kbO2"
   },
   "source": [
    "## 5 Diferenciación automática\n",
    "\n",
    "La [diferenciación automática](https://en.wikipedia.org/wiki/Automatic_differentiation)\n",
    "es una de las características más importantes de Tensorflow 2.x y es la base para poder aplicar\n",
    "en forma eficiente el algoritmo de [b-prop](https://en.wikipedia.org/wiki/Backpropagation) para el entrenamiento. Vamos a usar, para esto, las funciones de *GradientTape* [`tf.GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape?version=stable) para hacer el seguimiento de las operaciones que se realizan, con el fin de calcular los gradientes numéricos en forma eficiente.\n",
    "\n",
    "Cuando se realiza la propagación hacia adelante, todas las operaciones involucradas se graban en una cinta o *tape*. Una vez terminada la propagación hacia adelanta (*forward propagation*) la cinta se usa para calcular, hacia atrás (*backpropagation*). Por default, la cinta se borra una vez que se revisa hacia atras. Esto implica que un `tf.GradientTape` solo puede calcularse una sola vez. Si se llama de nuevo, la función va a regresar un error de ejecución, a menos que a la hora de crear el `tf.GradientTape` este sea definido como ```persistent```.\n",
    "\n",
    "Para ver como funciona el `tf.GradientTape` vamos a calcular el gradiente de una función fácil de derivar a mano:\n",
    "\n",
    "$$y = x^2$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdkqk8pw5yJM"
   },
   "outputs": [],
   "source": [
    "### Calculo de gradientes con GradientTape ###\n",
    "\n",
    "# y = x^2\n",
    "# Ejemplo: x = 3.0, y = 9.0, dy/dx = 6\n",
    "\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "# Inicializa el gradient tape\n",
    "with tf.GradientTape() as tape:\n",
    "  # Define la función\n",
    "  y = x * x\n",
    "# Calcula con GradientTape el gradiente numérico\n",
    "dy_dx = tape.gradient(y, x)\n",
    "\n",
    "assert dy_dx.numpy() == 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhU5metS5xF3"
   },
   "source": [
    "Para entrenar redes neuronales, se utiliza la diferenciación automática para calcular los grdientes respecto a los parámetros (pesos y sesgos) y el método de descenso de gradiente estocástico (SGD) para optimizar alguna función de pérdida (loss).\n",
    "\n",
    "Ahora que ya vimos como funciona el `GradientTape` para calcular y tener acceso a las derivadas, vamos a realizar un ejemplo donde usemos la diferenciación automática y el SGD para encontrar un valor de la variable $x$ que minimice la función de pérdida \n",
    "$$ L = (x - x_f)^2,$$\n",
    "donde $x_f$ es el valor que queremos encontrar.\n",
    "\n",
    "Claramente este problema lo podemos resolver en forma analítica ($x^* = x_f$), pero en este caso lo importante es mostrar como se puede utilizar el `GradientTape` para poder aprender y luego generalizarlo a una red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [
      "py"
     ],
     "id": ""
    },
    "id": "7g1yWiSXqEf-"
   },
   "outputs": [],
   "source": [
    "### Minimization con diferenciación automática y SGD ###\n",
    "\n",
    "# Inicializa un valor aleatorio para el valor inicial de x\n",
    "x = tf.Variable([tf.random.normal([1])])\n",
    "print(\"Initializing x={}\".format(x.numpy()))\n",
    "\n",
    "learning_rate = 1e-2 # learning rate for SGD\n",
    "history = []\n",
    "# Define the target value\n",
    "x_f = 4\n",
    "\n",
    "# Vamos a ejecutar el algoritmo de SGD unas 50 iteraciones.\n",
    "for i in range(500):\n",
    "  with tf.GradientTape() as tape:\n",
    "    '''TODO: define la funcion de pérdida'''\n",
    "    loss = # TODO\n",
    "\n",
    "  # minimización de la pérdida con el gradiente\n",
    "  grad = tape.gradient(loss, x) \n",
    "  nueva_x = x - learning_rate * grad \n",
    "  x.assign(nueva_x) \n",
    "  history.append(x.numpy()[0])\n",
    "\n",
    "# Plot the evolution of x as we optimize towards x_f!\n",
    "plt.plot(history)\n",
    "plt.plot([0, 500],[x_f,x_f])\n",
    "plt.legend(('Predicted', 'True'))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('x value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC7czCwk3ceH"
   },
   "source": [
    "`GradientTape` ofrece un marco poderoso y flexible de diferenciación automática. \n",
    "\n",
    "Ahora vamos a pasar a ver como usamos Tensorflow para resolver un problema concreto."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "WBk0ZDWY-ff8"
   ],
   "name": "Part1_TensorFlow.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "3233f9e61b2c212bd966ed82bdf879d60ca2ba5fe33e17cd0fe6d05850df9c31"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
