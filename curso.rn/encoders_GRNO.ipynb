{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "encoders_GRNO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "relevant-brooklyn"
      },
      "source": [
        "<center><H1>Modelado con RNN tipo <i>sequence to sequence</i> para la predicción de la demanda de energía eléctrica</H1><center>\n",
        "\n",
        "<center><img src=\"https://www.gstatic.com/devrel-devsite/prod/ve2848ad92313fddfcd40baeb58a2f663fe2fd55c371a714a6bb3e329e2b15223/tensorflow/images/lockup.svg\"  height=\"80px\" style=\"padding-bottom:5px;\"  /></center>\n",
        "\n",
        "<center><H2>Julio Waissman Vilanova</H2>"
      ],
      "id": "relevant-brooklyn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "consecutive-insured"
      },
      "source": [
        "<table align=\"center\">\n",
        "      <td align=\"center\"><a target=\"_blank\" href=\"https://www.unison.mx\">\n",
        "            <img src=\"https://www.unison.mx/wp-content/themes/awaken/images/logo.png\"  height=\"70px\" style=\"padding-bottom:5px;\"  /></a></td>  \n",
        "      <td align=\"center\"><a target=\"_blank\" href=\"https://www.gob.mx/cenace\">\n",
        "            <img src=\"https://universidad.cenace.gob.mx/pluginfile.php/244/block_html/content/CENACE-logo-completo.png\" width=\"300\" style=\"padding-bottom:5px;\" /></a></td>\n",
        "      <td align=\"center\"><a target=\"_blank\" href=\"https://colab.research.google.com/github/juliowaissman/rn-cenace/blob/main/encoders_GRNO.ipynb\">\n",
        "            <img src=\"https://i.ibb.co/2P3SLwK/colab.png\"  style=\"padding-bottom:5px;\" />Ejecuta en Google Colab</a></td>\n",
        "\n",
        "</table>"
      ],
      "id": "consecutive-insured"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "happy-kitty"
      },
      "source": [
        "\n",
        "# Las bibliotecas de base\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Para normalizar los datos de entrada\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "#Tensorflow con keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Gráficas más fáciles de manipular con plotly\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Como se verán las gráficas de matplotlib\n",
        "plt.style.use('ggplot')\n",
        "plt.rcParams['figure.figsize'] = (15,7)"
      ],
      "id": "happy-kitty",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "religious-therapy"
      },
      "source": [
        "## 1. Introducción\n",
        "\n",
        "En esta libreta vamos a desarrollar pasito a pasito un modelo para la predicción en redes neuronales utilizando un modelo de secuencia a secuencia.\n",
        "\n",
        "En un modelo de secuencia a secuencia, se utilizan básicamente dos redes, la primera recibe y procesa los datos de entrada (los cuales van a ser una serie de datos que entran a lo largo del tiempo). Una vez que se procesan los datos, estos se introducen en otra red (via un *vector de contexto*) para generar una secuencia de datos de estimación.\n",
        "\n",
        "Básicamente esto se logra con una *repeat vector layer* entre la red neuronal de procesamiento de la entrada (el *encoder*) y la red de salida (el *decoder*). \n",
        "\n",
        "Vamos a tratar de hacerlo parte por parte de manera que todo quede lo más claro posible. Así que haremos todo paso a paso.\n",
        "\n",
        "## 2. Descargar los datos\n",
        "\n",
        "Empecemos por cargar los datos:"
      ],
      "id": "religious-therapy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "classical-withdrawal"
      },
      "source": [
        "url = \"https://github.com/juliowaissman/curso-ml-cenace/raw/main/datos/Dataset_GCRNO_05052021.xlsx\"\n",
        "\n",
        "df = pd.read_excel(url, sheet_name='Datos')\n",
        "\n",
        "df_horario = df.melt(\n",
        "    id_vars= ['FECHA'],\n",
        "    value_vars= [f'DEM_GCRNO_H{i}' for i in range(24)],\n",
        "    var_name=\"Hora\",\n",
        "    value_name=\"Demanda\"\n",
        ").replace(\n",
        "    {f'DEM_GCRNO_H{i}': i for i in range(24)}\n",
        ")\n",
        "\n",
        "df_horario.index = df_horario.FECHA + pd.to_timedelta(df_horario.Hora, unit='h')\n",
        "df_horario.sort_index(inplace=True)\n",
        "df_horario.drop(columns=['FECHA', 'Hora'], inplace=True)\n",
        "df_horario = df_horario.asfreq('H', method='pad')\n",
        "df_horario[\"Fecha\"] = df_horario.index\n",
        "df_horario[\"Dia\"] = df_horario.index.weekday\n",
        "df_horario[\"Hora\"] = df_horario.index.hour\n",
        "df_horario[\"Mes\"] = df_horario.index.month\n",
        "\n",
        "print(df_horario.info())\n",
        "\n",
        "fig = px.line(df_horario, x=\"Fecha\", y=\"Demanda\", title='Demanda de energía GRNO')\n",
        "fig.show()"
      ],
      "id": "classical-withdrawal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moral-trauma"
      },
      "source": [
        "## 3. Acondicionamiento de datos para el aprendizaje\n",
        "\n",
        "\n",
        "Vamos a utilizar los datos del 2021 para prueba y el resto de entrenamiento"
      ],
      "id": "moral-trauma"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "connected-smith"
      },
      "source": [
        "df_train = df_horario[df_horario.index.year < 2020]\n",
        "df_val = df_horario[df_horario.index.year == 2020]\n",
        "df_test = df_horario[df_horario.index.year == 2021]\n",
        "\n",
        "df_train.shape, df_val.shape, df_test.shape"
      ],
      "id": "connected-smith",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "satisfied-stevens"
      },
      "source": [
        "Debido al algoritmo de B-prop a través del tiempo, las RNN son muy sensibles a los valores de los datos de entrada (en particular las LSTMs) por lo que es importante normalizar los datos. Notese que vamos a ajustar el `scaler` con los datos de entrenamiento, y se usa el mismo para ajustar los datos de prueba.\n",
        "\n",
        "Vamos a generar los objetos `scaler` necesarios para escalar a todas las variables que estamos utilizando."
      ],
      "id": "satisfied-stevens"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muslim-field"
      },
      "source": [
        "train = df_train\n",
        "scalers = {}  # Un diccionario con los scalers\n",
        "\n",
        "for attr in ['Demanda', 'Mes', 'Dia', 'Hora']:\n",
        "  scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "  s_s = scaler.fit_transform(train[attr].values.reshape(-1,1))\n",
        "  scalers[attr] = scaler\n",
        "  train[attr] = s_s.ravel()\n",
        "\n",
        "train.info()"
      ],
      "id": "muslim-field",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9O6NzSytscA"
      },
      "source": [
        "y ahora vamos a escalar los datos de `val` y `test` con el `scaler` ajustado:"
      ],
      "id": "U9O6NzSytscA"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjJ2HBzP_YIV"
      },
      "source": [
        "val = df_val\n",
        "for attr in ['Demanda', 'Mes', 'Dia', 'Hora']:\n",
        "  scaler = scalers[attr]\n",
        "  s_s = scaler.transform(val[attr].values.reshape(-1,1))\n",
        "  val[attr] = s_s.ravel()\n",
        "\n",
        "val.info()"
      ],
      "id": "NjJ2HBzP_YIV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLcCjDhchBHh"
      },
      "source": [
        "test = df_test\n",
        "for attr in ['Demanda', 'Mes', 'Dia', 'Hora']:\n",
        "  scaler = scalers[attr]\n",
        "  s_s = scaler.transform(test[attr].values.reshape(-1,1))\n",
        "  test[attr] = s_s.ravel()\n",
        "\n",
        "test.info()"
      ],
      "id": "cLcCjDhchBHh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prostate-opening"
      },
      "source": [
        "Pero esto no es suficiente para poder entrenar una red neuronal. Para poder entrenar la red necesitamos convertir los datos en muestras de entradas con observaciones pasada y con salidas futuras. para ser usados en el entrenamiento.\n",
        "\n",
        "Vamos a hacer una función para esto:"
      ],
      "id": "prostate-opening"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "southern-explosion"
      },
      "source": [
        "def divide_series(series, n_pasado, n_futuro, n_salto, es_train=True):\n",
        "  \"\"\"\n",
        "  n_pasado: número de observaciones pasadas para el encoder \n",
        "  n_futuro: número de observaciones futuras\n",
        "  n_salto: a partir de donde empiezan a contar las observaciones futuras\n",
        "\n",
        "  \"\"\"\n",
        "  X, y = list(), list() # Vamos a crear listas y al final hacemos ndarrays\n",
        "  generador = range(len(series)) if es_train else range(0, len(series), n_futuro)\n",
        "  \n",
        "  for ini in generador:\n",
        "    fin_anterior = ini + n_pasado\n",
        "    fin_actual = fin_anterior + n_salto + n_futuro\n",
        "    if fin_actual > len(series):\n",
        "      break\n",
        "    pasado = series[ini: fin_anterior, :]\n",
        "    futuro = series[fin_anterior + n_salto: fin_actual, :]\n",
        "    X.append(pasado)\n",
        "    y.append(futuro)\n",
        "  return np.array(X), np.array(y)"
      ],
      "id": "southern-explosion",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suspended-aquarium"
      },
      "source": [
        "Para nuestro caso, vamos a usar (esto lo podemos cambiar después) 7 días de información pasada, vamos a estimar 24 horas futuras, y las vamos a estimar después de que pasen 12 horas."
      ],
      "id": "suspended-aquarium"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "british-exhaust"
      },
      "source": [
        "n_pasado = 24 * 7\n",
        "n_futuro = 24\n",
        "n_salto = 12"
      ],
      "id": "british-exhaust",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "documented-thread"
      },
      "source": [
        "y ahora vamos a generar nuestos conjuntos de aprendizaje:"
      ],
      "id": "documented-thread"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "european-hypothetical"
      },
      "source": [
        "# Atributos a utilizar\n",
        "nom_attr = ['Demanda', 'Mes', 'Dia', 'Hora']\n",
        "n_attr = len(nom_attr)\n",
        "\n",
        "X_train, y_train = divide_series(train[nom_attr].values, n_pasado, n_futuro, n_salto)\n",
        "X_val, y_val = divide_series(val[nom_attr].values, n_pasado, n_futuro, n_salto)\n",
        "X_test, y_test = divide_series(test[nom_attr].values, n_pasado, n_futuro, n_salto, es_train=False)\n",
        "\n",
        "\n",
        "# Reacomodar como un tensor de 3 dimensiones\n",
        "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_attr))\n",
        "X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], n_attr))\n",
        "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_attr))\n",
        "\n",
        "y_train = y_train.reshape((y_train.shape[0], y_train.shape[1], n_attr))\n",
        "y_val = y_val.reshape((y_val.shape[0], y_val.shape[1], n_attr))\n",
        "y_test = y_test.reshape((y_test.shape[0], y_test.shape[1], n_attr))\n",
        "\n",
        "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape\n"
      ],
      "id": "european-hypothetical",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "other-landing"
      },
      "source": [
        "## 4. Modelo neuronal\n",
        "\n",
        "El modelo que vamos a usar está dividio en dos partes: el *encoder* y el *decoder*. Vamos iniciando definiendo el *encoder*:"
      ],
      "id": "other-landing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "accessible-lounge"
      },
      "source": [
        "encoder_inputs = layers.Input(shape=(n_pasado, n_attr))\n",
        "\n",
        "encoder_l1 = layers.LSTM(100, return_state=True)\n",
        "encoder_outputs1 = encoder_l1(encoder_inputs)\n",
        "\n",
        "encoder_states1 = encoder_outputs1[1:]"
      ],
      "id": "accessible-lounge",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "innocent-therapy"
      },
      "source": [
        "y ahora al decoder:"
      ],
      "id": "innocent-therapy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forbidden-anatomy"
      },
      "source": [
        "decoder_rvec = layers.RepeatVector(n_futuro)\n",
        "decoder_inputs = decoder_rvec(encoder_outputs1[0])\n",
        "\n",
        "decoder_l1 = layers.LSTM(100, return_sequences=True)\n",
        "decoder_l1_output = decoder_l1(decoder_inputs, initial_state=encoder_states1)\n",
        "\n",
        "decoder_l2 = layers.TimeDistributed(layers.Dense(n_attr))\n",
        "decoder_outputs = decoder_l2(decoder_l1_output)"
      ],
      "id": "forbidden-anatomy",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3B5WiYv46Pm"
      },
      "source": [
        "y ahora juntamos los dos modelos (método funcional)"
      ],
      "id": "K3B5WiYv46Pm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfx8f9QD4_3P"
      },
      "source": [
        "modelo = keras.models.Model(encoder_inputs, decoder_outputs)\n",
        "modelo.summary()"
      ],
      "id": "cfx8f9QD4_3P",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chief-territory"
      },
      "source": [
        "## 5 Entrenando el modelo\n",
        "\n",
        "Vamos ahora a aprovechar para ver como se usa un método de calendarización de la tasa de aprendizaje\n"
      ],
      "id": "chief-territory"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXbRT95To0Cg"
      },
      "source": [
        "reduce_lr = keras.callbacks.LearningRateScheduler(lambda x: 1e-3 * 0.90 ** x)\n",
        "\n",
        "path_checkpoint = \"model_checkpoint.h5\"\n",
        "modelckpt_callback = keras.callbacks.ModelCheckpoint(\n",
        "    monitor=\"val_loss\",\n",
        "    filepath=path_checkpoint,\n",
        "    verbose=1,\n",
        "    save_weights_only=True,\n",
        "    save_best_only=True,\n",
        ")\n",
        "\n",
        "\n",
        "es_callback = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", \n",
        "    min_delta=0, \n",
        "    patience=5\n",
        ")\n",
        "\n",
        "modelo.compile(\n",
        "    optimizer=keras.optimizers.Adam(), \n",
        "    loss=\"mae\"\n",
        ")\n",
        "\n",
        "history = modelo.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=25,\n",
        "    validation_data=(X_val,y_val),\n",
        "    batch_size=32,\n",
        "    callbacks=[reduce_lr, es_callback, modelckpt_callback]\n",
        ")"
      ],
      "id": "vXbRT95To0Cg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egy4qet9Wnwo"
      },
      "source": [
        "Si tuviste que parar el entrenamiento, o simplemente prefieres no gastar tu cuota de GPU, lo puedes cargar desde el checkpoint."
      ],
      "id": "Egy4qet9Wnwo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nheK2UU3gooa"
      },
      "source": [
        "!curl -O https://github.com/juliowaissman/rn-cenace/raw/main/model_checkpoint.h5"
      ],
      "id": "nheK2UU3gooa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC1YgIMMW4JP"
      },
      "source": [
        "modelo.load_weights(\"drive/MyDrive/model_checkpoint_original.h5\")"
      ],
      "id": "nC1YgIMMW4JP",
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Utm5Br07iq_K"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "Utm5Br07iq_K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO-0AlcqYKyk"
      },
      "source": [
        "Vamos entonces a simular los valores de salida"
      ],
      "id": "MO-0AlcqYKyk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yt_K1mkIYQbc"
      },
      "source": [
        "y_est = modelo.predict(X_test)\n",
        "y_test[:,:,0].ravel().shape, y_est[:,:,0].ravel().shape, df_test.Fecha[n_pasado + n_salto:-12].shape"
      ],
      "id": "Yt_K1mkIYQbc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CiXEC8e-YmH4"
      },
      "source": [
        "y ahora vamos a convertir ambas series en un data frame, y vamos a aplicarles el scaler al revés:"
      ],
      "id": "CiXEC8e-YmH4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEhckhJRYsyU"
      },
      "source": [
        "scaler = scalers['Demanda']\n",
        "\n",
        "yr = scaler.inverse_transform(y_test[:,:,0].ravel().reshape(-1, 1))\n",
        "yh = scaler.inverse_transform(y_est[:,:,0].ravel().reshape(-1, 1))\n",
        "\n",
        "df_est = pd.DataFrame({\n",
        "    \"Real\": yr.ravel(),\n",
        "    \"Estimado\": yh.ravel(),\n",
        "    \"Fecha\": df_test.Fecha[n_pasado + n_salto:-12]     \n",
        "})"
      ],
      "id": "hEhckhJRYsyU",
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zuh7P5M-c3L6"
      },
      "source": [
        "Y por último vamos a graficar la salida estimada con plotly"
      ],
      "id": "zuh7P5M-c3L6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WikGVqj2c12r"
      },
      "source": [
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=df_est.Fecha, y=df_est.Estimado, name=\"Estimada\"))\n",
        "fig.add_trace(go.Scatter(x=df_est.Fecha, y=df_est.Real, name=\"Real\"))\n",
        "fig.update_layout(title=\"Estimación de la demanda\")\n",
        "fig.show()"
      ],
      "id": "WikGVqj2c12r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WA9wn3sIfvU"
      },
      "source": [
        "## Ejercicio\n",
        "\n",
        "Prueba con una red más compleja, por ejemplo (pero no necesariamente) esta:"
      ],
      "id": "1WA9wn3sIfvU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZY0JT2fIevr"
      },
      "source": [
        "encoder_inputs = layers.Input(shape=(n_pasado, n_attr))\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "encoder_l1 = layers.GRU(100, return_sequences=True, return_state=True)\n",
        "encoder_outputs1 = encoder_l1(encoder_inputs)\n",
        "encoder_states1 = encoder_outputs1[1:]\n",
        "\n",
        "encoder_l2 = layers.GRU(100, return_state=True)\n",
        "encoder_outputs2 = encoder_l2(encoder_outputs1[0])\n",
        "encoder_states2 = encoder_outputs2[1:]\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "repeat_vector = layers.RepeatVector(n_attr)\n",
        "decoder_inputs = repeat_vector(encoder_outputs2[0])\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "decoder_l1 = layers.GRU(100, return_sequences=True)\n",
        "decoder_output1 = decoder_l1(decoder_inputs, initial_state=encoder_states1)\n",
        "\n",
        "\n",
        "decoder_l2 = tf.keras.layers.GRU(100, return_sequences=True)\n",
        "decoder_output2 = decoder_l2(decoder_output1, initial_state=encoder_states2)\n",
        "\n",
        "\n",
        "decoder_l3 = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_attr))\n",
        "decoder_outputs = decoder_l3(decoder_output2)\n",
        "\n",
        "#-------------------------------------------------------------------------------\n",
        "\n",
        "modelo2 = keras.models.Model(encoder_inputs, decoder_outputs)\n",
        "modelo2.summary()"
      ],
      "id": "7ZY0JT2fIevr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L-NqL-pfNK8"
      },
      "source": [
        "Aqui está el esquemático de éste modelo:\n",
        "\n",
        "![](https://github.com/juliowaissman/rn-cenace/raw/main/modelo_alternativo.jpg)"
      ],
      "id": "2L-NqL-pfNK8"
    }
  ]
}