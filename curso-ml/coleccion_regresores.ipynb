{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "coleccion_regresores.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqoNbKYxsVyp"
      },
      "source": [
        "<center>\n",
        "<p><img src=\"https://www.gob.mx/cms/uploads/image/file/179499/outstanding_quienes-somos.jpg\" width=\"300\">\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "# Curso *Machine Learning con uso de pandas, scikit learn y libretas jupyter*\n",
        "\n",
        "# Probando diferentes métodos de regresión\n",
        "\n",
        "\n",
        "<p> Julio Waissman Vilanova </p>\n",
        "<p>\n",
        "<img src=\"https://identidadbuho.unison.mx/wp-content/uploads/2019/06/letragrama-cmyk-72.jpg\" width=\"80\">\n",
        "</p>\n",
        "</center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXiqbbXwslXP"
      },
      "source": [
        "## Creando los conjuntos de datos a utilizar\n",
        "\n",
        "Para este problema vamos a generar dos conjuntos de aprendizaje sintéticos que nos ayudarán a visualizar diferencias entre métodos. \n",
        "\n",
        "Es importante siempre tener en cuenta que, por cuestiones de visualización, sólo estamos poniendo una regresión en una sola dimensión.\n",
        "\n",
        "Como hay métodos que son más efectivos en varias dimensiones, vamos a probar utilizando un conjunto multidimensional de datos reales, entre los conjuntos de juguete que vienen en *scikit-learn*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZtsieAetNmw"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pl\n",
        "\n",
        "\n",
        "# Primer problema de regresión sintético\n",
        "x1_plot = np.linspace(0, 10, 100)\n",
        "y1_plot = x1_plot * np.sin(x1_plot)\n",
        "\n",
        "x1 = np.linspace(0, 10, 100)\n",
        "rng = np.random.RandomState(0)\n",
        "rng.shuffle(x1)\n",
        "x1 = np.sort(x1[:20])\n",
        "\n",
        "y1 = x1 * np.sin(x1)\n",
        "x1 = x1.reshape(-1, 1)\n",
        "\n",
        "pl.figure(figsize=(15, 7))\n",
        "pl.plot(x1_plot, y1_plot, label=\"Verdad\")\n",
        "pl.plot(x1, y1, 'o', label='Entrenamiento')\n",
        "pl.title(\"Primer conjunto de prueba\")\n",
        "pl.legend()\n",
        "pl.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFUWzp034KjU"
      },
      "source": [
        "# Segundo problema de regresión sintético\n",
        "x2_plot = np.linspace(0, 6, 100)\n",
        "y2_plot = np.sin(x2_plot) + np.sin(6 * x2_plot)\n",
        "\n",
        "rng = np.random.RandomState(1)\n",
        "x2 = np.linspace(0, 6, 100)\n",
        "\n",
        "y2 = np.sin(x2) + np.sin(6 * x2) + rng.normal(0, 0.1, x2.shape[0])\n",
        "x2 = x2.reshape(-1, 1)\n",
        "\n",
        "pl.figure(figsize=(15, 7))\n",
        "pl.plot(x2_plot, y2_plot, label=\"Verdad\")\n",
        "pl.plot(x2, y2, 'o', label='Entrenamiento')\n",
        "pl.title(\"Segundo conjunto de prueba\")\n",
        "pl.legend()\n",
        "pl.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Dl0SZH-4LWT"
      },
      "source": [
        "# Tercer problema, con datos reales en 10 dimensiones\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x, y = load_diabetes(return_X_y=True)\n",
        "x3, x3_plot, y3, y3_plot = train_test_split(x, y, test_size=20, random_state=3)\n",
        "\n",
        "print(f\"El conjunto de entrenamiento tiene una forma {x3.shape}\")\n",
        "print(f\"El conjunto de validación tiene una forma {x3_plot.shape}\")\n",
        "\n",
        "pl.figure(figsize=(15, 7))\n",
        "pl.plot(y3_plot, 'ob', label=\"Verdad\")\n",
        "pl.title(\"Salidas del conjunto de prueba\")\n",
        "pl.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8GEizQt7sp3"
      },
      "source": [
        "## Seleccionando los métodos de regresión que vamos a probar\n",
        "\n",
        "Para este problema vamos a revisar diferentes métodos, los cuales vamos a iniciar con los parámetros ajustados con los valores básicos de la literatura, y poco a poco podremos irlos personalizando.\n",
        "\n",
        "Estos son los métodos que vamos a utilizar:\n",
        "\n",
        "**Aqui vamos a modificar si queremos probar que pasa modificando parámetros a los métodos o cambiando de método**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tn4PGy6z8IGr"
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "\n",
        "nombres = ['Rigida', 'Polinomial', 'SVR Lineal', 'SVR Gauss', 'SVR Poli', 'KNN', \n",
        "           'Arbol desición', 'Bósque aleatorio', 'GTB']\n",
        "regresores = [\n",
        "  Ridge(),\n",
        "  make_pipeline(PolynomialFeatures(3), Ridge()),\n",
        "  SVR(kernel='linear'),\n",
        "  SVR(kernel='rbf'),\n",
        "  SVR(kernel='poly'),\n",
        "  KNeighborsRegressor(),\n",
        "  DecisionTreeRegressor(),\n",
        "  RandomForestRegressor(),\n",
        "  GradientBoostingRegressor()\n",
        "]\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYcPMPZSAFGm"
      },
      "source": [
        "## Probando con los métodos seleccionados para regresión\n",
        "\n",
        "Ahora vamos a aplicar los mñetodos a los datos y vamos a medir la bondad de los metodos usando la medida MAE (Mean Absolute Error). \n",
        "\n",
        "Veamos que pasa con los diferentes métodos:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI1LA7-sAdNH"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "\n",
        "pl.figure(figsize=(15,45))\n",
        "\n",
        "pl.subplot(len(regresores) + 1, 3, 1)\n",
        "pl.plot(x1_plot, y1_plot, label=\"Verdad\")\n",
        "pl.plot(x1, y1, 'o', label='Entrenamiento')\n",
        "pl.title(r\"$x \\sin(x)$\")\n",
        "\n",
        "pl.subplot(len(regresores) + 1, 3, 2)\n",
        "pl.plot(x2_plot, y2_plot, label=\"Verdad\")\n",
        "pl.plot(x2, y2, 'o', label='Entrenamiento')\n",
        "pl.title(r\"$\\sin(x) + 6 \\sin(x)$\")\n",
        "\n",
        "pl.subplot(len(regresores) + 1, 3, 3)\n",
        "pl.plot(y3_plot, 'o', label='Datos de prueba')\n",
        "pl.title(\"Problema de diabetes (10 variables)\")\n",
        "\n",
        "for i, regr in enumerate(regresores):\n",
        "  regr.fit(x1, y1)\n",
        "  y1_est = regr.predict(x1_plot.reshape(-1,1))\n",
        "  mae = mean_absolute_error(y1_plot, y1_est)\n",
        "  pl.subplot(len(regresores) + 1, 3, 3 * (i + 1) + 1)\n",
        "  pl.plot(x1_plot, y1_est)\n",
        "  pl.plot(x1, y1, 'o')\n",
        "  pl.title(f\"{nombres[i]} (MAE: {mae:.2f})\")\n",
        "\n",
        "  regr.fit(x2, y2)\n",
        "  y2_est = regr.predict(x2_plot.reshape(-1,1))\n",
        "  mae = mean_absolute_error(y2_plot, y2_est)\n",
        "  pl.subplot(len(regresores) + 1, 3, 3 * (i + 1) + 2)\n",
        "  pl.plot(x2_plot, y2_est)\n",
        "  pl.plot(x2, y2, 'o')\n",
        "  pl.title(f\"{nombres[i]} (MAE: {mae:.2f})\")\n",
        "\n",
        "  regr.fit(x3, y3)\n",
        "  y3_est = regr.predict(x3_plot)\n",
        "  mae = mean_absolute_error(y3_plot, y3_est)\n",
        "  pl.subplot(len(regresores) + 1, 3, 3 * (i + 1) + 3)\n",
        "  pl.plot(y3_plot, 'o')\n",
        "  pl.plot(y3_est, 'x')\n",
        "  pl.title(f\"{nombres[i]} (MAE: {mae:.2f})\")\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znjQwG-JWQBw"
      },
      "source": [
        "## Vamos ahora a practicar\n",
        "\n",
        "1. Revisa en la documentación de *Scikit learn* las especificaciones y los parámetros que se pueden ajustar por cada uno de estos regresores. \n",
        "\n",
        "2. Prueba ajustar a cada uno de los problemas ¿Que encontraste? ¿Cuales son tus allazgos principales? ¿Que pasa cuando ajustas parámetros a un problema, con los otros problemas?\n",
        "\n",
        "3. Registra el mejor regresor, con los mejores parámetros para cada problema."
      ]
    }
  ]
}